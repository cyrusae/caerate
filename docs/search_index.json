[["index.html", "PLS/E-Rate wrangling overview 1 About this document", " PLS/E-Rate wrangling overview Cyrus Eosphoros 1 About this document This is a collection of notes and commented code that serves–or at least tries to serve–as an overview of the work I’ve done with TASCHA over the past year. This means there are several genres of topic being covered: data, as in information about the nature of the PLS (Public Library Survey) and E-Rate data; tooling, as in findings and suggestions for script-level changes and upgrades to the existing scripts; scripting, as in features/code I have written and why; architecture, plans for long-term development and explanations of why they’re like that from my perspective It is written in a conversational style because that was more conducive to getting more information down in place as opposed to less information. Thanks in advance for bearing with me on that front. "],["logic.html", "2 Logic 2.1 Contents 2.2 Back up a second 2.3 Important things to know 2.4 So: packages", " 2 Logic 2.1 Contents The original working code I have to date was formatted as an R package. This is inspired by an encounter with David Neuzerling’s arguments for package-driven development in data science. The main point of appeal for me here is portability–I wanted to be able to hand it off to other people with minimal preparation. Ideally, devtools would allow for installing the entire thing from GitHub instead of sending scripts around, e.g. on the actual server. 2.2 Back up a second At the time I’m writing this TASCHA’s scripts run on AWS (specifically, on a Linux EC2 instance). This is not directly comparable to the laptop I work on even if I do have a Linux server locally. One of my goals has been minimizing the extent to which that is noticeable. Couple key points for the data engineering/data science intersection that’s been happening here: From my perspective this has been an ETL pipeline problem. ETL stands for Extract/Transform/Load. It’s a very generalized way of describing the idea of a process that pulls data from one or more sources, cleans/structures/munges/otherwise changes it to be more usable for the entity doing the pulling, and puts it (the structured data) somewhere for safekeeping. The iSchool brought in Informatics students last year to make a data warehouse: a structured database for this data so it doesn’t have to live in CSVs or be computed on the fly. (UW custom/convenience means that the null hypothesis is for that database to be built in Microsoft SQL Server, but that’s not hugely relevant to this document.) Problem: in order to do the second thing, one must first have structured data to work with. I have been focused on step one for the duration. 2.2.1 Second-guessing One project-level note: what I eventually concluded was that it would’ve been ideal to come at this from two sides at the same time–make a temporary warehouse for the data TASCHA has now so that it’s more accessible for less computing overhead, and improve the overall pipeline process. This is, for logistical and division of labor reasons, not what I started off doing. It is what I would recommend and/or attempt to do now, given either time travel abilities or the ability to keep working on it. 2.3 Important things to know Recurring theme of things that we want to know whenever we’re dealing with data from my perspective: Are there unique identifiers for some kind of entity that aren’t dependent on human error? Where do they live? What values are NA in disguise? Are there fields that are many repetitions of a small number of repeating values? SQL is much stricter about data type and cleaning than R. Both R and SQL are good at filtering large amounts of data based on a single unique identifier if you can dig one up. 2.4 So: packages My work to date has been in a package I called HideousLaughter (it’s a Dungeons &amp; Dragons pun). The IMLS doesn’t provide a public API for the PLS data so the first thing I wanted to do was have an automated–so self-replicating–way to download the PLS result files so they’d be the same every time. Existing code also contains faster E-Rate downloading. The biggest unfinished project was sparked by the realization that I wasn’t comfortable making anyone depend on me (human, can misread things) transcribing metadata about hundreds of columns if there were places I could automate R doing it for me. The resulting flat file helper tool is always going to have some TASCHA genes from my perspective and will be available for anyone/everyone once it exists whether or not I’m here. Here is the entire package on GitHub. It is very much a “work-in-progress scratchpad” kind of situation. To the point where you could check out /inst for progress reports/trial and error/notes (and the summer branch for variations). Ideally you won’t have to. Following sections will explain some of the immediately-usable code. require(curl) #better downloads ## Loading required package: curl ## Using libcurl 7.81.0 with OpenSSL/3.0.2 require(data.table) #big data wrangling ## Loading required package: data.table require(magrittr) #use the pipe ## Loading required package: magrittr require(stringr) #wrangle strings ## Loading required package: stringr require(assertthat) #enforce expected outputs ## Loading required package: assertthat require(here) #file path management (for now; rprojroot preferable) ## Loading required package: here ## here() starts at /home/watcher/GitHere/caerate require(withr) #tempdir management; could also switch to withr pipe ## Loading required package: withr require(dplyr) #binding rows ## Loading required package: dplyr ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## between, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union ### replace with poorman! require(furrr) #simultaneity ## Loading required package: furrr ## Loading required package: future "],["pls-data.html", "3 PLS data 3.1 Some libraries here and why to use them 3.2 PLS download and cleaning utilities", " 3 PLS data Important things to know: PLS data has idiosyncratic flags for blanks/NAs that will bias how R interprets columns of their data. Their filenames and the structure of the .zips the IMLS releases can vary from year to year. The website they’re on is pretty stable but doesn’t have a machine-accessible API. 3.1 Some libraries here and why to use them furrr: helps with performance and making code legible by making it easier to run the same functions on a vector in parallel. rvest: scrapes the underlying HTML of the IMLS public-facing site so we can find the links we need. assertthat: this enforces data being a certain “shape” data.table: makes everything much faster. Some caveats: The code as written uses here, which is a package intended for interactive use. It should be replaced with rprojroot in practice. I was using base download.file here; curl is faster and cleaner. 3.2 PLS download and cleaning utilities Comments with #' are taking advantage of a Roxygen documentation skeleton (see arguments for package-shaped development). Key functions from the pls.R file: #&#39; Retrieve URLs from IMLS #&#39; #&#39; @description #&#39; Hits the IMLS page for PLS data dumps and returns a vector of URLs we want to download. #&#39; #&#39; @details #&#39; Future-proofing concerns: this works as long as the IMLS keeps putting things on their website as it worked in 2023 and don&#39;t come up with a new name scheme for files. If they do the latter, consider searching for a) links to zip files and b) links closest to the string &#39;CSV&#39;--it works now because the CSVs are the first/default option. Their file name scheme has been consistent since 2014, which happily enough is the timeframe TASCHA wants anyway, but if you want to go back further the parameter to change is `grepl` (consider trying `&#39;*.zip&#39;`, maybe). #&#39; #&#39; @param url Full address of the page on the IMLS site to retrieve download URLs from. Currently `&#39;https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey&#39;`. (Is a parameter to allow for unit testing, if this changes we probably have bigger problems.) #&#39; @param site IMLS site the download URLs expect as a prefix (`&#39;https://www.imls.gov&#39;`). Above parenthetical applies! #&#39; @param xpath Node from the IMLS page to look for results in. At time of writing the one that works is `&#39;//*[@data-ui-role=&quot;accordion&quot;]&#39;`. Used as `xpath` input for `rvest`. #&#39; @param element Element to retrieve contents of. Used as input for `rvest`. Default `&#39;a&#39;` (we are looking for links). #&#39; @param grepl Regex identifying a file that&#39;s relevant. Default `&#39;*pls_fy&#39;` (returns 2014-present because that happens to be how long they&#39;ve been consistently using that). #&#39; @param extract Regex to determine name scheme for FY extraction. Default `&#39;fy20..&#39;` (produces results like `&#39;fy2045&#39;`). #&#39; #&#39; @returns A named character vector of URLs to download and their corresponding reporting years. #&#39; @export #&#39; get_pls_urls &lt;- \\(url = &#39;https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey&#39;, site = &#39;https://www.imls.gov&#39;, xpath = &#39;//*[@data-ui-role=&quot;accordion&quot;]&#39;, element = &#39;a&#39;, grepl = &#39;*pls_fy&#39;, extract = &#39;fy20..&#39;) { # TODO validate inputs on principle pls &lt;- rvest::read_html(url) %&gt;% rvest::html_nodes(xpath = xpath) %&gt;% rvest::html_children() %&gt;% #the node contains years rvest::html_element(element) %&gt;% #get the first link rvest::html_attrs() #get the url that link refers to pls &lt;- pls[grepl(grepl, pls)] #reduce to real links pls &lt;- paste0(site, pls) #list of download URLs for zip files names(pls) &lt;- stringr::str_extract(pls, extract) #list now has name of the FY each URL is for pls #return list (character vector) of URLs with their FYs as names } This part is important: replaces things that aren’t real values according to the PLS documentation with NAs that R can recognize. #&#39; @param file Path to file (inherited from wrapper function). #&#39; @param response Either &#39;outlet&#39; or &#39;admin&#39;. #&#39; @param fy Expects `fy20..` format used elsewhere. #&#39; @param here See general concerns with `here` usage. #&#39; #&#39; @returns The filepath for the single successfully written file. #&#39; @export #&#39; get_pls_csv &lt;- \\(file, response, fy, here) { assertthat::is.readable(file) #is the file readable? assertthat::is.writeable(here::here(here)) #is the destination writable? assertthat::is.string(fy) #is the input fy coherent? assertthat::is.string(response) #do we know what the response is? dest &lt;- paste0(here::here(here), &#39;/pls_&#39;, response, &#39;_&#39;, fy, &#39;.csv&#39;) dt &lt;- data.table::fread(file = file) dt[dt == -9] &lt;- NA #Remove suppressed data dt[dt == -4] &lt;- NA #Remove for closures dt[dt == -3] &lt;- NA #Remove for closures dt[dt == -1] &lt;- NA #Remove unanswered questions dt[dt == &#39;M&#39;] &lt;- NA #Remove missing values if (&#39;MICROF&#39; %in% names(dt)) dt[MICROF == &#39;N&#39;, MICROF := NA] #NA for the MICROF field only if (&#39;RSTATUS&#39; %in% names(dt)) dt[RSTATUS == 3, RSTATUS := NA] #remove nonrespondents data.table::fwrite(dt, file = dest) assertthat::is.readable(path = dest) dest #return successfully-written file } The idiosyncratic PLS equivalents of NA: Negative numbers -9, -4, -3, and -1, which flag data suppressed for privacy reasons (libraries too small to be de-identified), temporary and permanent closures, and unanswered questions The letter “M”, which is what they actually use for NA per se “N”, but only in the MICROF column The number 3 (closures), but only in the RSTATUS column Clearing these as soon as possible gets our columns in (hopefully) the right data type and means we can use the NAs to identify rows we don’t care about going forward (e.g. nonrespondents one year won’t have anything to tell us we didn’t see last year). PLS-specific problem: a single year comes in a zip file that has three CSVs in it… somewhere. Possibly in a nested folder. With nonstandard filenames. But: one is always library outlets and is the longest, one is always state-level data and is the shortest, and one is always in the middle. We can work with that! Caveats about curl and here notwithstanding, these functions download files from the IMLS website, identify which CSV is which (we care about outlets and administrative entities), and save them in a more predictable structure. #&#39; Retrieve CSVs from IMLS zip file #&#39; #&#39; @description #&#39; Download a single zip file from the IMLS website (if needed), extract only the contents that are CSV files, identify and rename the outlet and administrative entity PLS responses, and return the paths to those files while deleting the intermediary files. #&#39; #&#39; @details #&#39; Development concerns: Current use of `here` is more brittle than I want it to be but I haven&#39;t figured out what the better long-term way to handle that is. #&#39; #&#39; @param url URL leading to a single zip of CSV files on the IMLS website (see `get_pls_urls()`). #&#39; @param extract Regex to determine name scheme for FY extraction. Default `&#39;fy20..&#39;` (produces results like `&#39;fy2045&#39;`). #&#39; @param here String describing the intended destination path. Default `&#39;data/raw/PLS_csvs&#39;`. Needs refinement as a feature. #&#39; #&#39; @returns A named character vector of length 2 containing the paths to the admin and outlet PLS responses. #&#39; @export #&#39; get_pls_zip &lt;- \\(url = url, extract = &#39;fy20..&#39;, here = &#39;data/raw/PLS_csvs&#39;) { assertthat::is.string(url) #Did we get a URL that we can ping assertthat::is.string(here) #Is the desired path viable if (is.null(names(url))) { fy &lt;- stringr::str_extract(url, extract) assertthat::assert_that(length(fy) == length(extract)) names(url) &lt;- fy } fy &lt;- names(url) #Get the FY value out fp &lt;- paste0(here::here(here), &#39;/&#39;, fy) if (!dir.exists(fp)) dir.create(fp) #Create directory if needed #Note: still deciding whether we&#39;re keeping the zip files and deleting everything else or what #TODO Add error handling once we&#39;re sure of cleanup steps assertthat::is.writeable(fp) #Can we write to the directory zipfile &lt;- paste0(fp, &#39;/&#39;, fy, &#39;.zip&#39;) if (!file.exists(zipfile)) { download.file(url = url, destfile = zipfile, quiet = TRUE) } #Download a file if it doesn&#39;t appear to exist yet assertthat::is.readable(zipfile) #Did we get the zip successfully (or have it already)? zip_contents &lt;- grep(&#39;\\\\w+\\\\.csv$&#39;, #find only the CSV files unzip(zipfile = zipfile, list = TRUE)$Name, #Names only ignore.case = TRUE, value = TRUE) unzip(zipfile = zipfile, files = zip_contents, exdir = fp) #put the CSV files in the /fy20XX/ directory zip_contents &lt;- list.files(path = fp, pattern = &#39;\\\\w+\\\\.csv$&#39;, full.names = TRUE, recursive = TRUE, include.dirs = TRUE) assertthat::assert_that(length(zip_contents) == 3) #make sure there are specifically three CSV files here zip_nrows &lt;- check_nrows(files = zip_contents) zip_results &lt;- data.table::data.table( path = zip_contents, nrows = zip_nrows ) #track features about the files that we&#39;ll need in a bit zip_results &lt;- zip_results[nrows != min(nrows), ] #remove the states file from consideration zip_results[nrows == max(nrows), response := &#39;outlet&#39;] #largest file will be outlets zip_results[nrows == min(nrows), response := &#39;admin&#39;] #remaining will be administrative entities csvs &lt;- furrr::future_map2_chr(.x = zip_results$path, .y = zip_results$response, .f = get_pls_csv, fy = fy, here = here) # Clean up process_files &lt;- list.files(path = fp, full.names = TRUE) %&gt;% setdiff(zipfile) #Get everything but the original zip unlink(process_files, recursive = TRUE) #Delete csvs #return paths to created files } #&#39; Use list of PLS URLs #&#39; #&#39; @param pls Character vector of PLS URLs (see `get_pls_urls()`); expects to find names in `fy20..` format also. #&#39; @param extract Fallback regex to get the names described above. #&#39; @param here String describing the intended destination path. Default `&#39;data/raw/PLS_csvs&#39;`. Needs refinement as a feature. #&#39; #&#39; @return Returns a list of the filenames being produced. #&#39; @export #&#39; get_pls_data &lt;- \\(pls, extract = &#39;fy20..&#39;, here = &#39;data/raw/PLS_csvs&#39;) { files &lt;- furrr::future_map(.x = pls, .f = get_pls_data, extract = extract, here = here) } The utility functions check_nrow and check_nrows were custom (in R/utils.R): #&#39; Number of rows in a fread-legible file #&#39; #&#39; @description #&#39; Uses `data.table::fread()` to determine `nrow` for a file without actually loading all of its contents. #&#39; #&#39; @param file Path to the file being evaluated. #&#39; #&#39; @returns Returns an integer `nrow` value. #&#39; @export #&#39; check_nrow &lt;- \\(file = filename) { assertthat::is.readable(file) #check filename n &lt;- data.table::fread(file = file, select = 1L) %&gt;% nrow() } #&#39; Vectorized `check_nrow()` #&#39; #&#39; @description #&#39; Uses `data.table::fread()` to determine `nrow` for multiple files without actually loading their contents. Returns a vector of results in the original order. #&#39; #&#39; @param files Character vector of one or more file paths. (If it&#39;s only one, you should probably be using `check_nrow()` on its own.) #&#39; #&#39; @returns Returns an integer vector corresponding to the `nrow` results in order. #&#39; @export #&#39; check_nrows &lt;- \\(files = c()) { assertthat::assert_that(is.vector(files, mode = &#39;character&#39;)) ns &lt;- furrr::future_map_int(.x = files, .f = check_nrow) } "],["e-rate-data.html", "4 E-Rate data 4.1 Socrata and performance 4.2 Why is all of the above important? 4.3 What next?", " 4 E-Rate data The most important thing I can tell you about USAC’s E-Rate data is this: what they refer to as a BEN (Billed Entity Number) is a unique identifier used for both administrative entities and beneficiaries. One library/system/etc. might acquire more than one BEN because someone responsible for filing the paperwork forgot theirs, but they are never reassigned. This liberates us from human typos if we play our cards right. The second most important thing I can tell you is that if you approach the data by way of the right dataset NIFs don’t seem to matter. (This is somewhat inside baseball; the purpose of it being laid out in this document is to explain to existing TASCHA folks what can change, not what NIFs are and why they’re a problem.) 4.1 Socrata and performance USAC uses Socrata for their open data APIs. Socrata has an R package. Socrata’s R package’s performance, benchmark-wise, is… bad. It’s really bad. The good news is, in the time since the original E-Rate dashboard, they changed their rate-limiting so that the staggered loops in the original download script aren’t necessary. (I also have never in my life been able to get USAC to recognize an app token in a request, but the good news is I’ve never when testing this sent it enough requests at once to need to.) Requests to the USAC API now allow arbitrary row limits but you do have to specify something–there’s not a parameter that lets you select infinite ones. You will be seeing a lot of $limit=1000000000 for getting around that on my part. Important note: from everything I could verify in the E-Rate data, all libraries apply for funding as either “Library” or “Library System”. Both of those are characteristics that can be filtered for in the first request. I used the C2 budget tool data to verify this. The general strategy here is to start with the smallest possible dataset that might have unique identifiers we want and use that to filter everything else as aggressively as possible. Here is the verification: #test filtering by url libraries &lt;- &#39;https://opendata.usac.org/resource/6brt-5pbv.csv?applicant_type=Library&amp;$limit=100000000&#39; lib_test &lt;- curl::curl_download(url = libraries, destfile = &#39;libtest.csv&#39;) fread(file = lib_test) ## ben billed_entity_name city ## 1: 125597 HASTINGS PUBLIC LIBRARY HASTINGS ## 2: 125848 THOMAS BEAVER FREE LIBRARY DANVILLE ## 3: 124826 NORTH TONAWANDA PUBLIC LIBRARY N TONAWANDA ## 4: 158659 JOHN B. CURTIS FREE PUBLIC LIBRARY BRADFORD ## 5: 17029547 BLUE LAKE RANCHERIA SYLVIA DANIELS LIBRARY BLUE LAKE ## --- ## 4354: 71374 NORTH SUBURBAN PUBLIC LIBRARY DISTRICT ROSCOE ROSCOE ## 4355: 145632 DELTA COMMUNITY LIBRARY DELTA JUNCTION ## 4356: 125701 JOSEPH T SIMPSON PUBLIC LIBRARY MECHANICSBURG ## 4357: 120709 ATTLEBORO PUBLIC LIBRARY ATTLEBORO ## 4358: 145570 KENNY LAKE PUBLIC LIBRARY COPPER CENTER ## state applicant_type ## 1: PA Library ## 2: PA Library ## 3: NY Library ## 4: ME Library ## 5: CA Library ## --- ## 4354: IL Library ## 4355: AK Library ## 4356: PA Library ## 4357: MA Library ## 4358: AK Library ## consulting_firm_name_crn ## 1: ## 2: TL TECH Services, LLC (17033105) ## 3: ## 4: ## 5: ## --- ## 4354: E-Rate Central (16060891); E-Rate Funding Solutions LLC (16070846) ## 4355: DeAnne Rand, E-Rate Coordinator (17018547); Valerie Oliver (17002356) ## 4356: ## 4357: B&amp;R Erate Consulting Services (17028255) ## 4358: DeAnne Rand, E-Rate Coordinator (17018547); Valerie Oliver (17002356) ## c2_budget_cycle child_entity_count c2_budget_algorithm full_time_students ## 1: FY2021-2025 0 Independent Library NA ## 2: FY2021-2025 0 Independent Library NA ## 3: FY2021-2025 0 Independent Library NA ## 4: FY2021-2025 0 Independent Library NA ## 5: FY2021-2025 0 Independent Library NA ## --- ## 4354: FY2021-2025 0 Independent Library NA ## 4355: FY2021-2025 0 Independent Library NA ## 4356: FY2021-2025 0 Independent Library NA ## 4357: FY2021-2025 0 Independent Library NA ## 4358: FY2021-2025 0 Independent Library NA ## library_square_footage school_student_multiplier library_multiplier ## 1: 2187 NA 4.5 ## 2: 8137 NA 4.5 ## 3: 26500 NA 4.5 ## 4: 1000 NA 4.5 ## 5: 200 NA 4.5 ## --- ## 4354: 22090 NA 4.5 ## 4355: 5000 NA 4.5 ## 4356: 16800 NA 4.5 ## 4357: 37000 NA 4.5 ## 4358: 1440 NA 4.5 ## c2_budget c2_budget_version funded_c2_budget_amount ## 1: 25000.0 Forecast 0.00 ## 2: 36616.5 Forecast 0.00 ## 3: 119250.0 Forecast 0.00 ## 4: 25000.0 Forecast 0.00 ## 5: 25000.0 Forecast 0.00 ## --- ## 4354: 99405.0 Confirmed 83287.81 ## 4355: 25000.0 Confirmed 8370.84 ## 4356: 75600.0 Confirmed 3476.85 ## 4357: 166500.0 Confirmed 50325.92 ## 4358: 25000.0 Confirmed 1680.00 ## pending_c2_budget_amount available_c2_budget_amount ## 1: 0 25000.00 ## 2: 0 36616.50 ## 3: 0 119250.00 ## 4: 0 25000.00 ## 5: 0 25000.00 ## --- ## 4354: 0 16117.19 ## 4355: 0 16629.16 ## 4356: 0 72123.15 ## 4357: 0 116174.08 ## 4358: 0 23320.00 #verified: this retrieves 4,347 results and USAC site expects the same. lib_sys_test &lt;- curl::curl_download(&#39;https://opendata.usac.org/resource/6brt-5pbv.csv?applicant_type=Library%20System&amp;$limit=1000000000&#39;, destfile = &#39;systest.csv&#39;) fread(file = lib_sys_test) ## ben billed_entity_name city state ## 1: 142853 SAN JUAN COUNTY LIBRARY SYSTEM BLANDING UT ## 2: 16075292 CLEARVIEW LIBRARY DISTRICT WINDSOR CO ## 3: 128867 PENDLETON COUNTY PUBLIC LIBRARY FALMOUTH KY ## 4: 16062599 CONTRA COSTA COUNTY LIBRARY MARTINEZ CA ## 5: 129119 WORTHINGTON PUBLIC LIBRARY WORTHINGTON OH ## --- ## 1447: 225808 FORT BRANCH - JOHNSON TOWNSHIP PUBLIC LIBRARY FORT BRANCH IN ## 1448: 150949 MASON COUNTY DISTRICT LIBRARY LUDINGTON MI ## 1449: 17000364 CENTRO DE SERVICIOS FERRÁN PONCE PR ## 1450: 130432 LA GRANGE COUNTY LIBRARY LAGRANGE IN ## 1451: 128791 BATH COUNTY MEMORIAL LIBRARY DISTRICT OWINGSVILLE KY ## applicant_type consulting_firm_name_crn ## 1: Library System UETN NFP E-Rate Consulting Services (17028946) ## 2: Library System ## 3: Library System ## 4: Library System EdTechnologyFunds, Inc. (16060507) ## 5: Library System Education Plus (16075832); OHIO E-RATE SERVICES (16062044) ## --- ## 1447: Library System AdTec-Administrative and Technical Consulting (16024741) ## 1448: Library System Elite Fund Inc (16043589) ## 1449: Library System ## 1450: Library System AdTec-Administrative and Technical Consulting (16024741) ## 1451: Library System ## c2_budget_cycle child_entity_count ## 1: FY2021-2025 4 ## 2: FY2021-2025 1 ## 3: FY2021-2025 2 ## 4: FY2021-2025 27 ## 5: FY2021-2025 5 ## --- ## 1447: FY2021-2025 2 ## 1448: FY2021-2025 2 ## 1449: FY2021-2025 2 ## 1450: FY2021-2025 4 ## 1451: FY2021-2025 4 ## c2_budget_algorithm full_time_students ## 1: Library System Wide (Aggregate Funding Floor) NA ## 2: Per Site Basis Library System NA ## 3: Per Site Basis Library System NA ## 4: Library System Wide NA ## 5: Per Site Basis Library System NA ## --- ## 1447: Library System Wide (Aggregate Funding Floor) NA ## 1448: Per Site Basis Library System NA ## 1449: Library System Wide (Aggregate Funding Floor) NA ## 1450: Per Site Basis Library System NA ## 1451: Per Site Basis Library System NA ## library_square_footage school_student_multiplier library_multiplier ## 1: 1350 NA 4.5 ## 2: 17827 NA 4.5 ## 3: 13770 NA 4.5 ## 4: 372309 NA 4.5 ## 5: 82854 NA 4.5 ## --- ## 1447: 5031 NA 4.5 ## 1448: 21228 NA 4.5 ## 1449: 850 NA 4.5 ## 1450: 44634 NA 4.5 ## 1451: 12309 NA 4.5 ## c2_budget c2_budget_version funded_c2_budget_amount ## 1: 50000.0 Forecast 0.00 ## 2: 80221.5 Confirmed 21538.73 ## 3: 86650.0 Confirmed 12124.11 ## 4: 1675390.5 Confirmed 512059.13 ## 5: 421731.5 Forecast 0.00 ## --- ## 1447: 50000.0 Confirmed 26485.58 ## 1448: 95526.0 Confirmed 20789.60 ## 1449: 50000.0 Confirmed 11039.76 ## 1450: 211691.5 Confirmed 37254.31 ## 1451: 125517.0 Confirmed 34479.50 ## pending_c2_budget_amount available_c2_budget_amount ## 1: 0.00 50000.00 ## 2: 5782.15 52900.62 ## 3: 0.00 74525.89 ## 4: 0.00 1163331.37 ## 5: 0.00 421731.50 ## --- ## 1447: 0.00 23514.42 ## 1448: 0.00 74736.40 ## 1449: 0.00 38960.24 ## 1450: 0.00 174437.19 ## 1451: 0.00 91037.50 #retrieves 1,448 library systems #USAC site browser also currently expects 1,448 library systems This is the approach I used to download “Library” and “Library System” data separately and then bind them together: curl::curl_download(url = &#39;https://opendata.usac.org/resource/6brt-5pbv.csv?applicant_type=Library&amp;$limit=1000000000&#39;, destfile = &#39;lib.csv&#39;) curl::curl_download(url = &#39;https://opendata.usac.org/resource/6brt-5pbv.csv?applicant_type=Library%20System&amp;$limit=1000000000&#39;, destfile = &#39;sys.csv&#39;) lib &lt;- data.table::fread(file = &#39;lib.csv&#39;) sys &lt;- data.table::fread(file = &#39;sys.csv&#39;) assertthat::are_equal(names(lib), names(sys)) #check that columns match ## [1] TRUE libsys &lt;- dplyr::bind_rows(lib, sys) assertthat::are_equal((nrow(lib) + nrow(sys)), nrow(libsys)) #check that the combined table contains rows from each ## [1] TRUE Note: dplyr is, for lack of a better word, a very “heavy” package to use for a single bind_rows. It can be substituted with the drop-in replacement poorman, which I strongly recommend. 4.2 Why is all of the above important? We can identify libraries and library systems without depending on regex matching. Libraries and library systems have permanent identities on the USAC side. Looping over a subset of the USAC data is no longer necessary. Basically everything I do when filtering data ends up depending on data.table joins. Here is that package’s documentation. It follows the same underlying logic as an SQL database when matching on key columns–which is a way of taking a shortcut past “searching” per se. (Think of it like overlapping stencils?) 4.3 What next? USAC uses the same unique keys across different datasets (e.g., BENs), but the column names vary; that’s something that requires human intervention to identify fully. But as soon as we have a list of the keys we’re interested in (e.g., the BEN for every entry in the C2 budget data that has to do with libraries), we can use those to filter down other datasets. The approach you would want would be downloading the entirety of a specific dataset of interest (curl to get a CSV and then data.table::fread is faster than streaming the data and means no surprises as far as how R interprets the data) and immediately filtering it down to only rows that include identities we know we care about (above). Repeat this process/logic to, e.g., find all applications that are funding a library, then find only line numbers for those applications. That’s the rest of what I was aiming for here. "],["architecture.html", "5 Architecture 5.1 Second drafts", " 5 Architecture The actual intended outcome here is for TASCHA folks to be able to a) do more data science on the available data and b) feed it into an R Shiny dashboard. Both of those things want to be powered by a database when able. My goals have been: Download the files we’re starting with, in the same way and the same place every time Predictable cleaning/filtering Save the filtered/clean versions, all in R–this also gives multiple places for a script to stop and check whether it’s doing anything new. Later we want to Load new data into the database Add or update existing tables in our canonical database as conservatively as possible. 5.1 Second drafts The R package targets exists to streamline exactly this kind of process. …But it’s only usable once you actually have the process running. One of my long-term goals was/is to take advantage of targets instead of having all the attempted avoiding of duplicated labor be manual. I was introduced to targets via this ode to its predecessor drake, which makes a good case for why something occupying that space is desirable. From what I can tell, TASCHA’s AWS installation has access to Glue. Glue can trigger some actions (like, “run script X when a new file is added to folder Y”) that we can’t daisy-chain together in R alone. So when there is an actual database to load CSVs into that would probably be how to prompt it to do so. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
